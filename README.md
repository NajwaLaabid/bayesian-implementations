# Basic Bayesian Implementations

This repository contains basic implementations of bayesian-related concepts/tools, mainly in Python and related libraries. This is an ongoing project that will expand as I explore more Bayesian methods/concepts through courses and personal readings.

## Gaussian Mixture Model Generative Process

**Notebook**: `test-gmm.ipynb`

An implementation of the bayesian version of a [Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) in Python 3.

## Gibbs Sampler 

**Notebook**: `test-gibbs-sampler.ipynb`

An implementation of [Gibbs Sampler](https://en.wikipedia.org/wiki/Gibbs_sampling#:~:text=In%20statistics%2C%20Gibbs%20sampling%20or,when%20direct%20sampling%20is%20difficult.) in Tensorflow Probability. The implementation can be found in `gibbs_sampler_utils.py`. The sampler is evaluated on simulated data (generated by the function `gmm()` in `utils.py`) and the real-world task of inferring the length distributions of two groups of tank-grown and wild fish (data given in `fish_data.txt`, original study can be found [here](https://www.sciencedirect.com/science/article/abs/pii/S0044848616304835)).

## Empirical Proof of Central Limit Theorem

**Notebook**: `test-clt.ipynb`

An empirical proof of Central Limit Theorem using Tensorflow Probability. This is a simple practice exerice with Tensorflow Probability. We show that the means of a Bernoulli and Uniform distributions converge to a Normal distirbutions with a large enough sample size. The code for plotting is available in utils.py and the experiments can be run on the notebook experiments.ipynb.

## Exploring JAX framework 

**Notebook**: `test-jax.ipynb`

This is a basic exploration of [JAX](https://github.com/google/jax) framework. The code is mainly adapted from Robert Tjarko Lange [tutorial](https://roberttlange.github.io/posts/2020/03/blog-post-10/). The project is currently work in progress (comments on the notebook coming up soon).