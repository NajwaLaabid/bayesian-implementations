{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP in JAX\n",
    "\n",
    "Exploring the basic concepts of JAX by building a Multi-Layer Perceptron. This notebook follows Robert Lange's [tutorial](https://roberttlange.github.io/posts/2020/03/blog-post-10/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional JAX and dataloader helpers\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "from utils import jax_utils as helpers\n",
    "from jax import grad, jit, vmap, value_and_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch data loader. Any other data loader with similar functionality will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "helpers.plot_mnist_examples(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define JAX functions needed for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is a functional programming framework. We need to replace PyTorch modules and class instances with functions to be able to use JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elaanaj/miniconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "# initialize the weights of the layers\n",
    "\n",
    "key = random.PRNGKey(1)\n",
    "\n",
    "def initialize_mlp(sizes, key):\n",
    "    \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "    keys = random.split(key, len(sizes))\n",
    "    # Initialize a single layer with Gaussian weights -  helper function\n",
    "    def initialize_layer(m, n, key, scale=1e-2):\n",
    "        w_key, b_key = random.split(key)\n",
    "        return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "    return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "params = initialize_mlp(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9399388ce414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Make a batched version of the `predict` function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbatch_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vmap' is not defined"
     ]
    }
   ],
   "source": [
    "# prediction function\n",
    "def forward_pass(params, in_array):\n",
    "    \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "    activations = in_array\n",
    "    \n",
    "    # Loop over the ReLU hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        activations = relu_layer([w, b], activations)\n",
    "    \n",
    "    # Perform final trafo to logits\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "def loss(params, in_arrays, targets):\n",
    "    \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
    "    preds = batch_forward(params, in_arrays)\n",
    "    return -np.sum(preds * targets)\n",
    "  \n",
    "def accuracy(params, data_loader):\n",
    "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
    "    acc_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = np.array(data).reshape(data.size(0), 28*28)\n",
    "        targets = one_hot(np.array(target), num_classes)\n",
    "    \n",
    "        target_class = np.argmax(targets, axis=1)\n",
    "        predicted_class = np.argmax(batch_forward(params, images), axis=1)\n",
    "        acc_total += np.sum(predicted_class == target_class)\n",
    "    return acc_total/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to update the parameters of the model\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "\n",
    "# Defining an optimizer in Jax\n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 10\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to train the MLP model using the JAX functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "    # Initialize placeholder for loggin\n",
    "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "    \n",
    "    # Get the initial set of parameters \n",
    "    params = get_params(opt_state)\n",
    "    \n",
    "    # Get initial accuracy after random init\n",
    "    train_acc = accuracy(params, train_loader)\n",
    "    test_acc = accuracy(params, test_loader)\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_acc_test.append(test_acc)\n",
    "    \n",
    "    # Loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if net_type == \"MLP\":\n",
    "                # Flatten the image into 784 vectors for the MLP\n",
    "                x = np.array(data).reshape(data.size(0), 28*28)\n",
    "            elif net_type == \"CNN\":\n",
    "                # No flattening of the input required for the CNN\n",
    "                x = np.array(data)\n",
    "            y = one_hot(np.array(target), num_classes)\n",
    "            params, opt_state, loss = update(params, x, y, opt_state)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = accuracy(params, train_loader)\n",
    "        test_acc = accuracy(params, test_loader)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                    train_acc, test_acc))\n",
    "    \n",
    "    return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                          opt_state,\n",
    "                                                          net_type=\"MLP\")\n",
    "\n",
    "# Plot the loss curve over time\n",
    "helpers.plot_mnist_performance(train_loss, train_log, test_log,\n",
    "                       \"MNIST MLP Performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
